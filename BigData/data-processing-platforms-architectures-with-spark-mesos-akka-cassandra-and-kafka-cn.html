<!DOCTYPE html>
<!-- saved from url=(0105)http://datastrophic.io/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>数据处理平台架构SMACK: Spark, Mesos, Akka, Cassandra and Kafka 中文版</title>
    <meta name="description" content="">

    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="shortcut icon" href="http://datastrophic.io/favicon.ico">

    <link rel="stylesheet" type="text/css" href="./data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-cn_files/screen.css">
    <link rel="stylesheet" type="text/css" href="./data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-cn_files/css">
    <link rel="stylesheet" type="text/css" href="./data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-cn_files/prism.css">

    <link rel="canonical" href="http://datastrophic.io/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka/">
    
    <meta property="og:site_name" content="datastrophic">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Data processing platforms architectures with SMACK: Spark, Mesos, Akka, Cassandra and Kafka">
    <meta property="og:description" content="This post is a follow-up of the talk given at Big Data AW meetup in Stockholm and focused on different use cases and design approaches for building scalable data processing platforms with SMACK(Spark, Mesos, Akka, Cassandra, Kafka) stack. While...">
    <meta property="og:url" content="http://datastrophic.io/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka/">
    <meta property="article:published_time" content="2015-09-16T22:51:12.322Z">
    <meta property="article:modified_time" content="2016-03-27T09:12:02.664Z">
    <meta property="article:tag" content="Cassandra">
    <meta property="article:tag" content="Mesos">
    <meta property="article:tag" content="Akka">
    <meta property="article:tag" content="Spark">
    <meta property="article:tag" content="Kafka">
    <meta property="article:tag" content="SMACK">
    
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Data processing platforms architectures with SMACK: Spark, Mesos, Akka, Cassandra and Kafka">
    <meta name="twitter:description" content="This post is a follow-up of the talk given at Big Data AW meetup in Stockholm and focused on different use cases and design approaches for building scalable data processing platforms with SMACK(Spark, Mesos, Akka, Cassandra, Kafka) stack. While...">
    <meta name="twitter:url" content="http://datastrophic.io/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka/">
    
    <script async="" src="./data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-cn_files/analytics.js"></script><script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "Article",
    "publisher": "datastrophic",
    "author": {
        "@type": "Person",
        "name": "Anton Kirillov",
        "image": "//www.gravatar.com/avatar/935d478a02639e34cd9d753e81436b4e?s=250&d=mm&r=x",
        "url": "http://datastrophic.io/author/akirillov",
        "sameAs": null,
        "description": "Distributed systems engineer building systems based on Cassandra/Spark/Mesos stack. Huge Scala/Akka fan."
    },
    "headline": "Data processing platforms architectures with SMACK: Spark, Mesos, Akka, Cassandra and Kafka",
    "url": "http://datastrophic.io/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka/",
    "datePublished": "2015-09-16T22:51:12.322Z",
    "dateModified": "2016-03-27T09:12:02.664Z",
    "keywords": "Cassandra, Mesos, Akka, Spark, Kafka, SMACK",
    "description": "This post is a follow-up of the talk given at Big Data AW meetup in Stockholm and focused on different use cases and design approaches for building scalable data processing platforms with SMACK(Spark, Mesos, Akka, Cassandra, Kafka) stack. While..."
}
    </script>

    <meta name="generator" content="Ghost 0.6">
    <link rel="alternate" type="application/rss+xml" title="datastrophic" href="http://datastrophic.io/rss/">
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-65032691-1', 'auto');
  ga('send', 'pageview');

</script>
<style id="fit-vids-style">.fluid-width-video-wrapper{width:100%;position:relative;padding:0;}.fluid-width-video-wrapper iframe,.fluid-width-video-wrapper object,.fluid-width-video-wrapper embed {position:absolute;top:0;left:0;width:100%;height:100%;}</style></head>
<body class="post-template tag-cassandra tag-mesos tag-akka tag-spark tag-kafka tag-smack nav-closed">

    

    <div class="site-wrapper">

        


<header class="main-header post-head no-cover">
    <nav class="main-nav  clearfix">
        
    </nav>
</header>

<main class="content" role="main">
    <article class="post tag-cassandra tag-mesos tag-akka tag-spark tag-kafka tag-smack">

        <header class="post-header">
            <h1 class="post-title">数据处理平台架构SMACK: Spark, Mesos, Akka, Cassandra and Kafka 中文版</h1>
            <section class="post-meta">
                <time class="post-date" datetime="2015-09-16">16 September 2015</time>  on <a href="http://datastrophic.io/tag/cassandra/">Cassandra</a>, <a href="http://datastrophic.io/tag/mesos/">Mesos</a>, <a href="http://datastrophic.io/tag/akka/">Akka</a>, <a href="http://datastrophic.io/tag/spark/">Spark</a>, <a href="http://datastrophic.io/tag/kafka/">Kafka</a>, <a href="http://datastrophic.io/tag/smack/">SMACK</a>
            </section>
        </header>

        <section class="post-content">
            <p>这篇博客是<a href="http://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka">talk given at Big Data AW meetup</a>这篇文章的后续， 并且关注于不同的用例及设计方法，从而使用SMACK(Spark, Mesos, Akka, Cassandra, Kafka) Stack构建多个可扩展的数据处理平台。 当stack足够简洁并且只包含少量的组件，它就有可能实现不同的系统设计，这样就可以既有批处理，又有流处理。 但是也会有更加复杂的Lambda架构和Kappa架构。所以让我们通过概述快速的达成理解上的一致，然后继续看产品项目经验里总结的设计和实例。</p>
			

<h6 id="recap">概述</h6>

<p><img src="./data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-cn_files/SMACK-Stack-Architectures---Google-Slides-2015-09-15-23-34-56.jpg" alt=""></p>

<ul>
<li><p><a href="http://spark.apache.org/">Spark</a> - 分布式、大规模数据处理的快速并且通用的引擎</p></li>
<li><p><a href="http://mesos.apache.org/">Mesos</a> - 集群资源管理系统，提供了高效的分布式应用程序的资源隔离和共享</p></li>
<li><p><a href="http://akka.io/">Akka</a> - 是一个工具包和运行时间，他能够在Java虚拟机上构建高并发，分布式和高容错的消息驱动应用。</p></li>
<li><p><a href="http://cassandra.apache.org/">Cassandra</a> - 分布式、高可用的数据库，设计用于处理多个数据中心的大量的数据</p></li>
<li><p><a href="http://kafka.apache.org/">Kafka</a> - 一种用于处理实时数据馈送的高吞吐量、低延迟的分布式发布订阅消息系统</p></li>
</ul>

<h6 id="storagelayercassandra">存储层: Cassandra</h6>

<p><img src="./data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-cn_files/Cassadnra-XDCR--2-.png" alt="">
Cassandra拥有着众所周知的高可用性高吞吐量的特征，并且能够处理大量的写入负载和生存集群节点故障。 在CAP定理条件中，Cassandra为操作提供了可调的一致性和可用性。</p>

<p>更有趣的是，当涉及到数据处理，Cassandra可以线性扩展（通过往集群增加节点来解决增大的负载），另外他还提供了跨数据中心(XDCR)复制的能力，实际上XDCR不仅提供了复制的能力，更多的提供了许多有趣的用例能够被使用:</p>

<ul>
<li>地理分布的数据中心可以按特定的区域或者按最靠近客户的区域来处理数据</li>
<li>跨数据中心的数据迁移：在出现故障后做恢复或将数据移动到一个新的数据中心</li>
<li>工作负载的操作和分析分离</li>
</ul>

<p>但是所有的功能都有他们自己的价值，对Cassandra来说，它的价值就是它的数据模型, 它可以被认为是作为嵌套排序图，分布在集群节点的按集群列排序或分组的分区键值和条目。接下来是一个小例子:</p>

<pre class=" language-sql"><code class=" language-sql"><span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> campaign<span class="token punctuation">(</span>  
  id uuid<span class="token punctuation">,</span>
  year <span class="token keyword">int</span><span class="token punctuation">,</span>
  month <span class="token keyword">int</span><span class="token punctuation">,</span>
  day <span class="token keyword">int</span><span class="token punctuation">,</span>
  views <span class="token keyword">bigint</span><span class="token punctuation">,</span>
  clicks <span class="token keyword">bigint</span><span class="token punctuation">,</span>
  <span class="token keyword">PRIMARY</span> <span class="token keyword">KEY</span> <span class="token punctuation">(</span>id<span class="token punctuation">,</span> year<span class="token punctuation">,</span> month<span class="token punctuation">,</span> day<span class="token punctuation">)</span>
<span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> campaign<span class="token punctuation">(</span>id<span class="token punctuation">,</span> year<span class="token punctuation">,</span> month<span class="token punctuation">,</span> day<span class="token punctuation">,</span> views<span class="token punctuation">,</span> clicks<span class="token punctuation">)</span>  
<span class="token keyword">VALUES</span><span class="token punctuation">(</span><span class="token number">40b08953</span><span class="token operator">-</span><span class="token number">a</span>…<span class="token punctuation">,</span><span class="token number">2015</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">,</span> <span class="token number">42</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token keyword">SELECT</span> views<span class="token punctuation">,</span> clicks <span class="token keyword">FROM</span> campaign  
<span class="token keyword">WHERE</span> id<span class="token operator">=</span><span class="token number">40b08953</span><span class="token operator">-</span><span class="token number">a</span>… <span class="token operator">and</span> year<span class="token operator">=</span><span class="token number">2015</span> <span class="token operator">and</span> month<span class="token operator">&gt;</span><span class="token number">8</span><span class="token punctuation">;</span>  
</code></pre>

<p>在一定范围内获得明确的数据，键值必须也是明确的，并在列表中的最后一列以外的任何范围子句都不允许。 此约束被引入到限制多个扫描不同的范围，这将产生随机访问磁盘和降低性能。这意味着要小心设计数据模型对阅读查询限制读/扫描的量，从而当它来支持新查询时导致较小的灵活性。<a href="http://slides.com/antonkirillov/cassandra-data-modelling-101#/">这是一些 C* 数据模型 101</a>  的幻灯片，提供了一些CQL表是如何被本质地体现的例子。</p>

<p>但是如果有一些表需要和其他一些表连接呢？让我们考虑下以下情况：计算给定月份里的所有campaigns中的每个campaign的总视图。 </p>

<pre class=" language-sql"><code class=" language-sql"><span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> event<span class="token punctuation">(</span>  
  id uuid<span class="token punctuation">,</span>
  ad_id uuid<span class="token punctuation">,</span>
  campaign uuid<span class="token punctuation">,</span>
  ts <span class="token keyword">bigint</span><span class="token punctuation">,</span>
  <span class="token keyword">type</span> <span class="token keyword">text</span><span class="token punctuation">,</span>
  <span class="token keyword">PRIMARY</span> <span class="token keyword">KEY</span><span class="token punctuation">(</span>id<span class="token punctuation">)</span>
<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>

<p>根据给定的模型，唯一的办法是读取所有的campaigns，读取所有的events，将符合要求的部分（匹配campaign id的）相加，并且指给campaign。由于存储在Cassandra里的数据量非常的大并不适合内存化，所以实施这样的应用程序非常有挑战性。因此处理这中数据就应该由一个分布式的方式来完成，而Spark则是一个完美的选择。 </p>

<h6 id="processinglayerspark">处理层: Spark</h6>

<p><img src="./data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-cn_files/Spark-Basics--RDD--DAG--Scheduler---1-.png" alt="">
Spark操作的主要抽象概念是RDD(弹性分布式数据集, 元素的一种分布式集合)， 工作流程主要分为四个部分:</p>

<ul>
<li>RDD 操作(变换和行动)形式DAG (无回路有向图)</li>
<li>DAG被拆分成多任务的多个阶段，然后提交到集群管理。</li>
<li>阶段结合了任务不需要重新分配。</li>
<li>任务跑在工人和结果上，然后返回给客户端</li>
</ul>

<p>下面演示Spark和Cassandra如何一起解决上面的问题的:  </p>

<pre class=" language-scala"><code class=" language-scala"><span class="token keyword">val</span> sc <span class="token operator">=</span> <span class="token keyword">new</span> SparkContext<span class="token punctuation">(</span>conf<span class="token punctuation">)</span>

<span class="token keyword">case</span> <span class="token keyword">class</span> Event<span class="token punctuation">(</span>id<span class="token operator">:</span> UUID<span class="token punctuation">,</span> ad_id<span class="token operator">:</span> UUID<span class="token punctuation">,</span> campaign<span class="token operator">:</span> UUID<span class="token punctuation">,</span> ts<span class="token operator">:</span> <span class="token builtin">Long</span><span class="token punctuation">,</span> `<span class="token keyword">type</span>`<span class="token operator">:</span> <span class="token builtin">String</span><span class="token punctuation">)</span>

sc<span class="token punctuation">.</span>cassandraTable<span class="token punctuation">[</span>Event<span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token string">"keyspace"</span><span class="token punctuation">,</span> <span class="token string">"event"</span><span class="token punctuation">)</span>  
  <span class="token punctuation">.</span>filter<span class="token punctuation">(</span>e <span class="token keyword">=&gt;</span> e<span class="token punctuation">.</span>`<span class="token keyword">type</span>` <span class="token operator">==</span> <span class="token string">"view"</span> <span class="token operator">&amp;&amp;</span> checkMonth<span class="token punctuation">(</span>e<span class="token punctuation">.</span>ts<span class="token punctuation">)</span><span class="token punctuation">)</span>
  <span class="token punctuation">.</span>map<span class="token punctuation">(</span>e <span class="token keyword">=&gt;</span> <span class="token punctuation">(</span>e<span class="token punctuation">.</span>campaign<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
  <span class="token punctuation">.</span>reduceByKey<span class="token punctuation">(</span>_ <span class="token operator">+</span> _<span class="token punctuation">)</span>
  <span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>

<p>通过<a href="https://github.com/datastax/spark-cassandra-connector">spark-cassandra-connector</a> ，Cassandra可以更加简单和直接的和Spark进行交互。 这里还有一个有趣的选项，将Spark和NoSQL存储一起工作 -- SaprkSQL，他将SQL声明转换成一系列的RDD运行。</p>

<pre class=" language-scala"><code class=" language-scala"><span class="token keyword">case</span> <span class="token keyword">class</span> CampaignReport<span class="token punctuation">(</span>id<span class="token operator">:</span> <span class="token builtin">String</span><span class="token punctuation">,</span> views<span class="token operator">:</span> <span class="token builtin">Long</span><span class="token punctuation">,</span> clicks<span class="token operator">:</span> <span class="token builtin">Long</span><span class="token punctuation">)</span>

sql<span class="token punctuation">(</span><span class="token string">"""SELECT campaign.id as id, campaign.views as views,  
   campaign.clicks as clicks, event.type as type
        FROM campaign
        JOIN event ON campaign.id = event.campaign
    """</span><span class="token punctuation">)</span><span class="token punctuation">.</span>rdd
<span class="token punctuation">.</span>groupBy<span class="token punctuation">(</span>row <span class="token keyword">=&gt;</span> row<span class="token punctuation">.</span>getAs<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token string">"id"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token punctuation">.</span>map<span class="token punctuation">{</span> <span class="token keyword">case</span> <span class="token punctuation">(</span>id<span class="token punctuation">,</span> rows<span class="token punctuation">)</span> <span class="token keyword">=&gt;</span>
   <span class="token keyword">val</span> views <span class="token operator">=</span> rows<span class="token punctuation">.</span>head<span class="token punctuation">.</span>getAs<span class="token punctuation">[</span><span class="token builtin">Long</span><span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token string">"views"</span><span class="token punctuation">)</span>
   <span class="token keyword">val</span> clicks <span class="token operator">=</span> rows<span class="token punctuation">.</span>head<span class="token punctuation">.</span>getAs<span class="token punctuation">[</span><span class="token builtin">Long</span><span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token string">"clicks"</span><span class="token punctuation">)</span>

   <span class="token keyword">val</span> res <span class="token operator">=</span> rows<span class="token punctuation">.</span>groupBy<span class="token punctuation">(</span>row <span class="token keyword">=&gt;</span> row<span class="token punctuation">.</span>getAs<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token string">"type"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mapValues<span class="token punctuation">(</span>_<span class="token punctuation">.</span>size<span class="token punctuation">)</span>
   CampaignReport<span class="token punctuation">(</span>id<span class="token punctuation">,</span> views <span class="token operator">=</span> views <span class="token operator">+</span> res<span class="token punctuation">(</span><span class="token string">"view"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> clicks <span class="token operator">=</span> clicks <span class="token operator">+</span> res<span class="token punctuation">(</span><span class="token string">"click"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token punctuation">}</span><span class="token punctuation">.</span>saveToCassandra<span class="token punctuation">(</span>“keyspace”<span class="token punctuation">,</span> “campaign_report”<span class="token punctuation">)</span>
</code></pre>

<p>通过几行代码，他就能实现本身很复杂的原生的Lamba设计，但是这个例子只是为了显示多么容易可以做到这一点。  </p>

<h6 id="almostmapreducebringingprocessingclosertodata">几乎映射归纳：让处理更接近数据</h6>

<p>Saprk-Cassandra 连接器是一个数据局部性感知及从集群最近的结点读取数据从而减少通过网络传输的数据量。为了全面支持Spark-C*连接器数据局部性感知，Spark工人需要布置在所有Cassandra结点上。<br>
<img src="./data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-cn_files/Cassadnra-Spark--5-.png" alt="">
除了Spark和Cassandra的搭配，也可以独立一个运作的（或者重写的）集群用作分析：</p>

<ul>
<li>集群可以相对独立的被缩放</li>
<li>数据由Cassandra来复制，没有额外的工作</li>
<li>用于分析的集群有不同的读/写负载模式 </li>
<li>用于分析的集群能包含额外的数据（如字典）和处理结果</li>
<li>Spark资源影响仅受限于一个集群上</li>
</ul>

<p>让我们再一次看看Spark应用部署选项: <br>
<img src="./data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-cn_files/cluster-overview.png" alt="">
集群资源管理器中提供了三个主要选项：</p>

<ul>
<li>Saprk 单机版 - Spark master和workers被安装和执行在一个独立的应用程序中（这很明显带来了一些ops开销，而且每个worker只支持静态资源分配）</li>
<li>如果你在用Hadoop生态系统，YARN是一个不错的选择</li>
<li>Mesos最初被设计用于动态分配集群资源，不仅为了跑Hadoop应用，而且还可以处理各种各样的负载。</li>
</ul>

<h6 id="mesosarchitecture">Mesos架构</h6>

<p><img src="./data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-cn_files/Mesos-Overview.png" alt="">
Mesos集群包含了多个负责提供资源和调度的Master结点和真正执行沉重任务的Slave结点。具有多个Masters ZooKeeper的HA模式是用于领导选举和服务发现。Mesos上执行的应用程序被称作框架，利用API来控制资源供应，并提交任务和Mesos。总体上任务执行流程包含以下步骤：</p>

<ul>
<li>Slaves 发布空闲的资源给Master</li>
<li>Master发送资源提供给框架</li>
<li>调度回应每个任务所需的任务和资源</li>
<li>Master发送任务给slaves</li>
</ul>

<h6 id="bringingsparkmesosandcassandratogether">Spark，Mesos和Cassandra一起工作</h6>

<p>如之前所说的，Saprk workers 应该配置在Cassandra结点上来加强数据局部性感知，这样可以降低网络传输量和Cassandra集群负载。下面是一个如何和Mesos一起实现的可行的部署场景。<br>
<img src="./data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-cn_files/Spark-Mesos-Cassandra--2-.png" alt=""></p>

<ul>
<li>Mesos Masters和ZooKeepers搭配</li>
<li>Mesos Slaves搭建在Cassandra结点上以加强Spark获得更好的数据局部性</li>
<li>Spark二进制部署在所有的worker结点上，<code>spark-env.sh</code>被配置了合适的master主终端和执行jar包路径</li>
<li>Saprk的执行JAR包上传到S3/HDFS</li>
</ul>

<p>通过提供安装Saprk的任务，在任何已经安装了Spark二进制文件以及包含实际工作逻辑的组件Jar包的worker上就可以调用简单的spark-submit命令来提交到集群 </p>

<pre class=" language-bash"><code class=" language-bash">spark-submit --class io.datastrophic.SparkJob /etc/jobs/spark-jobs.jar  
</code></pre>

<p>有一个已知的选项是运行Docker化的Spark，这样就不需要跨越每一个集群结点来分发二进制文件。</p>

<h6 id="scheduledandlongrunningtasksexecution">已计划的和长时间运行的任务执行</h6>

<p>每一个数据处理系统迟早会面对跑两种任务的必要性：已计划/周期性任务就像定期分批聚合，长时间运行的任务，如流处理的例子。这两种类型的主要需求是容错 - 即使在集群结点失败的情况下，任务必须继续运行。Mesos生态系统在两个强大的框架下支持任何一种类型。</p>

<p><strong>Marathon</strong>是一个为了长时间任务执行时容错的框架，他在ZooKeeper上支持HA模型，可以运行Docker，具有很好的REST API。下面是一个用于运行<code>spark-submit</code> shell命令的简单任务配置例子:
<img src="./data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-cn_files/Marathon-2015-09-08-22-13-39.jpg" alt=""></p>

<p><strong>Chronos</strong>拥有和Marathon一样的特性，但是它是为了运行已计划的作业而设计的，并且在总体上是分布式的HAcron, 支持作业图表。下面是一个作为简单批处理脚本来实现S3压实作业配置的一个例子：
<img src="./data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-cn_files/Chronos-2015-09-08-22-21-15.jpg" alt=""></p>

<p>目前已经有大量的框架存在，同时也有很多框架有针对性的集成了Mesos资源管理功能到广泛使用的系统正在开发中。下面是指列出了其中的一些：</p>

<ul>
<li>Hadoop</li>
<li>Cassandra</li>
<li>Kafka</li>
<li>Myriad: YARN on Mesos</li>
<li>Storm</li>
<li>Samza</li>
</ul>

<h6 id="ingestingthedata">摄取数据</h6>

<p>到目前为止一切都挺好：设计了数据层，设置了资源管理，配置好了作业。仅剩的事情就是没有处理数据。 <br>
<img src="./data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-cn_files/Architecture-Ingestion--2-.png" alt="">
假设输入的数据将很高几率到达终端，接收它将面临接下来的需求：</p>

<ul>
<li>提供高吞吐量/低延迟</li>
<li>有弹性</li>
<li>允许简易扩展</li>
<li>支持背压</li>
</ul>

<p>背压不是必须的，但是对于处理负载峰值最好有。</p>

<p>Akka完美地符合这些需求，基本上他就是被设计成提供这些功能集合的。所以什么是Akka：</p>

<ul>
<li>为JVM实现角色模型</li>
<li>基于消息和异步</li>
<li>加强没有分享的可变状态</li>
<li>将一个进程轻松扩展到机器集群</li>
<li>在父监督下，角色形成等级</li>
<li>不仅是并发的框架：akka-http, akka-streams, akka-persistence</li>
</ul>

<p>下面是一个关于三个角色的简单的例子，一个角色处理JSON Http请求，一个角色将请求解析成域名模型案例类，还有一个角色保存这个类到Cassandra: </p>

<pre class=" language-scala"><code class=" language-scala"><span class="token keyword">class</span> HttpActor <span class="token keyword">extends</span> Actor <span class="token punctuation">{</span>  
  <span class="token keyword">def</span> receive <span class="token operator">=</span> <span class="token punctuation">{</span>
    <span class="token keyword">case</span> req<span class="token operator">:</span> HttpRequest <span class="token keyword">=&gt;</span> 
      system<span class="token punctuation">.</span>actorOf<span class="token punctuation">(</span>Props<span class="token punctuation">[</span>JsonParserActor<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">!</span> req<span class="token punctuation">.</span>body
    <span class="token keyword">case</span> e<span class="token operator">:</span> Event <span class="token keyword">=&gt;</span>
      system<span class="token punctuation">.</span>actorOf<span class="token punctuation">(</span>Props<span class="token punctuation">[</span>CassandraWriterActor<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">!</span> e
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>

<span class="token keyword">class</span> JsonParserActor <span class="token keyword">extends</span> Actor <span class="token punctuation">{</span>  
  <span class="token keyword">def</span> receive <span class="token operator">=</span> <span class="token punctuation">{</span>
    <span class="token keyword">case</span> s<span class="token operator">:</span> <span class="token builtin">String</span> <span class="token keyword">=&gt;</span> Try<span class="token punctuation">(</span>Json<span class="token punctuation">.</span>parse<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">.</span>as<span class="token punctuation">[</span>Event<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">match</span> <span class="token punctuation">{</span>
      <span class="token keyword">case</span> Failure<span class="token punctuation">(</span>ex<span class="token punctuation">)</span> <span class="token keyword">=&gt;</span> <span class="token comment" spellcheck="true">//error handling code</span>
      <span class="token keyword">case</span> Success<span class="token punctuation">(</span>event<span class="token punctuation">)</span> <span class="token keyword">=&gt;</span> sender <span class="token operator">!</span> event
    <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>

<span class="token keyword">class</span> CassandraWriterActor <span class="token keyword">extends</span> Actor <span class="token keyword">with</span> ActorLogging <span class="token punctuation">{</span>  
  <span class="token comment" spellcheck="true">//for demo purposes, session initialized here</span>
  <span class="token keyword">val</span> session <span class="token operator">=</span> Cluster<span class="token punctuation">.</span>builder<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span>addContactPoint<span class="token punctuation">(</span><span class="token string">"cassandra.host"</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span>build<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span>connect<span class="token punctuation">(</span><span class="token punctuation">)</span>

  <span class="token keyword">override</span> <span class="token keyword">def</span> receive<span class="token operator">:</span> Receive <span class="token operator">=</span> <span class="token punctuation">{</span>
    <span class="token keyword">case</span> event<span class="token operator">:</span> Event <span class="token keyword">=&gt;</span>
      <span class="token keyword">val</span> statement <span class="token operator">=</span> <span class="token keyword">new</span> SimpleStatement<span class="token punctuation">(</span>event<span class="token punctuation">.</span>createQuery<span class="token punctuation">)</span>
        <span class="token punctuation">.</span>setConsistencyLevel<span class="token punctuation">(</span>ConsistencyLevel<span class="token punctuation">.</span>QUORUM<span class="token punctuation">)</span>

      Try<span class="token punctuation">(</span>session<span class="token punctuation">.</span>execute<span class="token punctuation">(</span>statement<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">match</span> <span class="token punctuation">{</span>
        <span class="token keyword">case</span> Failure<span class="token punctuation">(</span>ex<span class="token punctuation">)</span> <span class="token keyword">=&gt;</span> <span class="token comment" spellcheck="true">//error handling code</span>
        <span class="token keyword">case</span> Success <span class="token keyword">=&gt;</span> sender <span class="token operator">!</span> WriteSuccessfull
      <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre>

<p>看起来仅仅几行代码就能够让一切都工作，但是当用Akka轻松的写原始数据（事件）到Cassandra，也会有一些陷阱：</p>

<ul>
<li>Cassandra是被设计成快速服务而不是批量处理，所以输入数据的预聚合处理是很需要的</li>
<li>聚合/汇总 的计算时间将会随着数据量的大小而成长</li>
<li>由于无状态设计模式，角色不适合处理聚合</li>
<li>微批处理可能部分解决该问题</li>
<li>原始数据依然需要某些可靠的缓存</li>
</ul>

<h6 id="kafkaasabufferforincomingdata">Kafka作为输入数据的缓存</h6>

<p><img src="./data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-cn_files/Architecture-Ingestion-Kafka--1-.png" alt="">
为了保持输入的数据有一些保留，其进一步的预聚合/处理一些分布式提交日志可以被使用。在这里情况下，消费者将分批读取数据，处理成预聚合的形式并保存到Cassandra。 下面是一个利用akka-http发布从HTTP来的JSON数据到Kafak的例子：  </p>

<pre class=" language-scala"><code class=" language-scala"><span class="token keyword">val</span> config <span class="token operator">=</span> <span class="token keyword">new</span> ProducerConfig<span class="token punctuation">(</span>KafkaConfig<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  
<span class="token keyword">lazy</span> <span class="token keyword">val</span> producer <span class="token operator">=</span> <span class="token keyword">new</span> KafkaProducer<span class="token punctuation">[</span>A<span class="token punctuation">,</span> A<span class="token punctuation">]</span><span class="token punctuation">(</span>config<span class="token punctuation">)</span>  
<span class="token keyword">val</span> topic <span class="token operator">=</span> “raw_events”

<span class="token keyword">val</span> routes<span class="token operator">:</span> Route <span class="token operator">=</span> <span class="token punctuation">{</span>  
  post<span class="token punctuation">{</span>
    decodeRequest<span class="token punctuation">{</span>
      entity<span class="token punctuation">(</span>as<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span> str <span class="token keyword">=&gt;</span>
        JsonParser<span class="token punctuation">.</span>parse<span class="token punctuation">(</span>str<span class="token punctuation">)</span><span class="token punctuation">.</span>validate<span class="token punctuation">[</span>Event<span class="token punctuation">]</span> <span class="token keyword">match</span> <span class="token punctuation">{</span>
          <span class="token keyword">case</span> s<span class="token operator">:</span> JsSuccess<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span> <span class="token keyword">=&gt;</span> producer<span class="token punctuation">.</span>send<span class="token punctuation">(</span><span class="token keyword">new</span> KeyedMessage<span class="token punctuation">(</span>topic<span class="token punctuation">,</span> str<span class="token punctuation">)</span><span class="token punctuation">)</span>
          <span class="token keyword">case</span> e<span class="token operator">:</span> JsError <span class="token keyword">=&gt;</span> BadRequest <span class="token operator">-</span><span class="token operator">&gt;</span> JsError<span class="token punctuation">.</span>toFlatJson<span class="token punctuation">(</span>e<span class="token punctuation">)</span><span class="token punctuation">.</span>toString<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token punctuation">}</span>
      <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>    
<span class="token punctuation">}</span>

<span class="token keyword">object</span> AkkaHttpMicroservice <span class="token keyword">extends</span> App <span class="token keyword">with</span> Service <span class="token punctuation">{</span>  
  Http<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>bindAndHandle<span class="token punctuation">(</span>routes<span class="token punctuation">,</span> config<span class="token punctuation">.</span>getString<span class="token punctuation">(</span><span class="token string">"http.interface"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> config<span class="token punctuation">.</span>getInt<span class="token punctuation">(</span><span class="token string">"http.port"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token punctuation">}</span>
</code></pre>

<h6 id="consumingthedatasparkstreaming">消费数据： Spark Streaming</h6>

<p>当Akka还能被用作从Kafka消费流数据，在你的生态系统里有Spark带来的Spark Streaming可以作为解决问题的一个选项：</p>

<ul>
<li>它支持数据源的多样性</li>
<li>提供at-least-once语义</li>
<li>和Kafka Direct及幂等存储一起，exactly-once语义有效
<img src="./data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-cn_files/streaming-flow.png" alt=""></li>
</ul>

<p>从Kinesis来的消费事件流通过Spark Streaming的例子：  </p>

<pre class=" language-scala"><code class=" language-scala"><span class="token keyword">val</span> ssc <span class="token operator">=</span> <span class="token keyword">new</span> StreamingContext<span class="token punctuation">(</span>conf<span class="token punctuation">,</span> Seconds<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token keyword">val</span> kinesisStream <span class="token operator">=</span> KinesisUtils<span class="token punctuation">.</span>createStream<span class="token punctuation">(</span>ssc<span class="token punctuation">,</span>appName<span class="token punctuation">,</span>streamName<span class="token punctuation">,</span>  
   endpointURL<span class="token punctuation">,</span>regionName<span class="token punctuation">,</span> InitialPositionInStream<span class="token punctuation">.</span>LATEST<span class="token punctuation">,</span>     
   Duration<span class="token punctuation">(</span>checkpointInterval<span class="token punctuation">)</span><span class="token punctuation">,</span> StorageLevel<span class="token punctuation">.</span>MEMORY_ONLY<span class="token punctuation">)</span>
<span class="token punctuation">}</span>

<span class="token comment" spellcheck="true">//transforming given stream to Event and saving to C*</span>
kinesisStream<span class="token punctuation">.</span>map<span class="token punctuation">(</span>JsonUtils<span class="token punctuation">.</span>byteArrayToEvent<span class="token punctuation">)</span>  
<span class="token punctuation">.</span>saveToCassandra<span class="token punctuation">(</span>keyspace<span class="token punctuation">,</span> table<span class="token punctuation">)</span>

ssc<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>  
ssc<span class="token punctuation">.</span>awaitTermination<span class="token punctuation">(</span><span class="token punctuation">)</span>  
</code></pre>

<h6 id="designingforfailurebackupsandpatching">为失败做设计：备份和补丁</h6>

<p>通常这是任何系统最无聊的部分，但是当他有可能存在时它是非常重要的，进入系统的数据可能是无效的或者当所有分析数据中心都崩溃了。<br>
<img src="./data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-cn_files/Architecture-Ingestion-Kafka-HDFS.png" alt=""></p>

<p>所以为什么不存储数据在Kafka/Kinesis? 目前写在Kinesis的数据只保留一天，没有备份一旦发生失败，所有的处理结果都可能丢失。虽然Kafka支持更大保留周期，但是硬件所有权的成本需要被考虑，因为如S3存储的例子，和S3 SLA一样，更便宜比多个实例运行Kafka更好。</p>

<p>除了有备份恢复/修补策略，应该有前期的设计和测试，这样任何数据上的问题才能够很快被修复。程序员在聚合作业或复制数据上的错误会破坏计算结果的精确性，因此修复错误不应该成为大问题。通过在加强数据模型的幂等可以让所有的运行更简单，这样同一个运行的多数保留会产生相同结果。（举例：sql更新是幂等操作如果计数不增加）</p>

<p>下面是一个读取S3备份并加载到Cassandra的Spark业务的例子：  </p>

<pre class=" language-scala"><code class=" language-scala"><span class="token keyword">val</span> sc <span class="token operator">=</span> <span class="token keyword">new</span> SparkContext<span class="token punctuation">(</span>conf<span class="token punctuation">)</span>

sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span>s<span class="token string">"s3n://bucket/2015/*/*.gz"</span><span class="token punctuation">)</span>  
  <span class="token punctuation">.</span>map<span class="token punctuation">(</span>s <span class="token keyword">=&gt;</span> Try<span class="token punctuation">(</span>JsonUtils<span class="token punctuation">.</span>stringToEvent<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
  <span class="token punctuation">.</span>filter<span class="token punctuation">(</span>_<span class="token punctuation">.</span>isSuccess<span class="token punctuation">)</span><span class="token punctuation">.</span>map<span class="token punctuation">(</span>_<span class="token punctuation">.</span>get<span class="token punctuation">)</span>
  <span class="token punctuation">.</span>saveToCassandra<span class="token punctuation">(</span>config<span class="token punctuation">.</span>keyspace<span class="token punctuation">,</span> config<span class="token punctuation">.</span>table<span class="token punctuation">)</span>
</code></pre>

<h6 id="thebigpicture">大局观</h6>

<p>用SMACK打造数据平台构建的高层次设计。 <br>
<img src="./data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-cn_files/Full-Architecture-Ingestion-Kafka-HDFS--1--1.png" alt="">
总结，什么是SMACK栈：</p>

<ul>
<li>包含了各种各样数据处理方案的简单的工具箱</li>
<li>久经考验和广泛使用的具有大型社区的软件</li>
<li>易扩展性和数据复制，同时保持较低的延迟</li>
<li>统一的为异构负载提供集群管理</li>
<li>适合各种类型应用的单一平台</li>
<li>为不同架构设计的平台实现（batch, streaming, Lambda, Kappa）</li>
<li>非常快速的将产品推到市场 （举例：MVP验证）</li>
</ul>
        </section>

        <footer class="post-footer">


            <figure class="author-image">
                <a class="img" href="http://datastrophic.io/author/akirillov/" style="background-image: url(//www.gravatar.com/avatar/935d478a02639e34cd9d753e81436b4e?s=250&amp;d=mm&amp;r=x)"><span class="hidden">Anton Kirillov's Picture</span></a>
            </figure>

            <section class="author">
                <h4><a href="http://datastrophic.io/author/akirillov/">Anton Kirillov</a></h4>

                    <p>Distributed systems engineer building systems based on Cassandra/Spark/Mesos stack. Huge Scala/Akka fan.</p>
                <div class="author-meta">
                    
                    
                </div>
            </section>


            <section class="share">
                <h4>Share this post</h4>
                <a class="icon-twitter" href="https://twitter.com/share?text=Data%20processing%20platforms%20architectures%20with%20SMACK%3A%20Spark%2C%20Mesos%2C%20Akka%2C%20Cassandra%20and%20Kafka&amp;url=http://datastrophic.io/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka/" onclick="window.open(this.href, &#39;twitter-share&#39;, &#39;width=550,height=235&#39;);return false;">
                    <span class="hidden">Twitter</span>
                </a>
                <a class="icon-facebook" href="https://www.facebook.com/sharer/sharer.php?u=http://datastrophic.io/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka/" onclick="window.open(this.href, &#39;facebook-share&#39;,&#39;width=580,height=296&#39;);return false;">
                    <span class="hidden">Facebook</span>
                </a>
                <a class="icon-google-plus" href="https://plus.google.com/share?url=http://datastrophic.io/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka/" onclick="window.open(this.href, &#39;google-plus-share&#39;, &#39;width=490,height=530&#39;);return false;">
                    <span class="hidden">Google+</span>
                </a>
            </section>

        </footer>

    </article>
</main>

<aside class="read-next">
    <a class="read-next-story no-cover" href="http://datastrophic.io/core-concepts-architecture-and-internals-of-apache-spark/">
        <section class="post">
            <h2>Apache Spark: core concepts, architecture and internals</h2>
            <p>This post covers core concepts of Apache Spark such as RDD, DAG, execution workflow, forming stages of tasks and…</p>
        </section>
    </a>
    <a class="read-next-story prev no-cover" href="http://datastrophic.io/evaluating-cassandra-2-1-counters-consistency/">
        <section class="post">
            <h2>Cassandra 2.1 Counters: Testing Consistency During Nodes Failures</h2>
            <p>For some cases such as adserving counters are really handy to accumulate totals for events coming into a system…</p>
        </section>
    </a>
</aside>



        <footer class="site-footer clearfix">
            <section class="copyright"><a href="http://datastrophic.io/">datastrophic</a> © 2016</section>
            <section class="poweredby">Proudly published with <a href="https://ghost.org/">Ghost</a></section>
        </footer>

    </div>

    <script src="./data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-cn_files/jquery.min.js"></script>

    <script type="text/javascript" src="./data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-cn_files/jquery.fitvids.js"></script>
    <script type="text/javascript" src="./data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-cn_files/index.js"></script>
    <script type="text/javascript" src="./data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-cn_files/prism.js"></script>



</body></html>